# From Bookmark Chaos to 200+ Living Urdu Novel Goldmines  
A Personal Data Rescue Mission for Smart Urdu Novel Bank

Hey there ğŸ‘‹  

This isnâ€™t just a â€œWebLinks-Data-Cleaningâ€ readme.  

This is the story of how I turned several years of messy, joyful, obsessive bookmarking into something actually useful for thousands of Urdu novel lovers.

For many years I was that guy who â€” every time I found a beautiful old Urdu novel site, a rare Mediafire folder, or a hidden blog full of rare novels â€” quickly hit Ctrl+D and threw it into my already exploding Google Bookmarks folder called â€œNovelsâ€ (which, honestly, should have been named â€œNovels + Memes + Random YouTube + Allah knows what elseâ€).

Fast forward to 2025: Iâ€™m running [Smart Urdu Novel Bank](https://www.urdunovelbanks.com/2025/04/smart-urdu-novel-bank-ai-powered-novel.html) â†’ already hosting 70,000+ novels â€” and I desperately want to make Smart Urdu Novel Bank 10Ã— better. That means finding, scraping and bringing in thousands more novels from every corner of the internet.

But firstâ€¦ I had to clean up my own mess.

## The Real Starting Point

Exported ~4,000â€“5,000 bookmarks from many years â†’ got one giant HTML file â†’ parsed it into Google Sheets using Apps Script â†’ stared at thousands of rows thinking:  
â€œOkayâ€¦ now what?â€

Most links were:
- Facebook posts  
- Instagram reels  
- YouTube drama OSTs  
- Random forum threads  
- â€¦and yes, some actual gold â€” old Blogspot sites, personal hosting pages, Mediafire index pages full of novels

So I rolled up my sleeves.

## How I Actually Did It (The Human Way)

### Phase 1 â€“ The Orange Marker Era  
I opened the giant sheet and started scrollingâ€¦ and scrollingâ€¦ and scrolling.  
Every time I saw a link that screamed â€œUrdu novels live hereâ€, I painted the whole row **orange** (yes, with my mouse, like a 90s kid highlighting notes).

After 2â€“3 evenings of chai + scrolling, I had maybe 400â€“500 orange rows.

Then I wrote my first little script:  
â€œFind all orange rows â†’ copy them to a new sheet called Orange Dataâ€  

That felt like magic.

### Phase 2 â€“ Turning Ugly Deep Links into Clean Domains

Most bookmarks were not clean domains.  
They were:

https://novelbank.blogspot.com/2021/07/anarkali-novel-by-imran-series-complete.html?m=1&fbclid=IwAR3longgarbage  

So I learned just enough regex to save my life:
^https?://[^/]+


â€¦and slowly turned thousands of monster URLs into nice clean:

- rekhta.org  
- urdunovelbanks.com  
- zubi novels.blogspot.com  
- etc.

Also killed all the `www.`, forced everything to `https://`, removed trailing slashes â€” basically made them look like adults.

### Phase 3 â€“ The â€œAre You Even Alive?â€ Test

Here came the hardest (and most heartbreaking) part.

I wrote a script that politely knocks on every domain:

- 200 â†’ â€œHi, Iâ€™m alive and hosting novels ğŸ¥³â€  
- 404 â†’ â€œSorry broâ€¦ I died years ago ğŸ˜¢â€  
- 403 / 503 / timeout â†’ â€œIâ€™m either angry or sleeping, come back laterâ€

Google gave me only 6 minutes per run â†’ so I added â€œmemoryâ€:  
the script remembers which rows already have a status code and skips them next time.  
That little trick let me run it 7â€“8 times over two days until everything was checked.

Seeing 404 after 404 hurtâ€¦ but seeing 200 after a long-forgotten Blogspot link felt like finding buried treasure.

## Final Treasure Chest (January 2026)

After all the crying, chai, regex headaches and #REF! disasters:

- ~220 clean, living domains that actually host Urdu novels  
- Ready to be crawled for titles + Mediafire / GDrive links  
- Sorted, deduplicated, human-verified-ish


## The Real Tools I Used (My Script Arsenal)

Here are all the functions I actually ran â€” in roughly the order I used them:

1. **`copyOrangeTextRows()`**  
   â†’ My first hero script. Scanned the giant raw sheet, looked for my chosen orange text color (#FF9900 â€” after many failed attempts with wrong shades), and copied only those rows to a new sheet called **"Orange Data"**.  
   This saved me from manually copying 400â€“500 rows by hand.

2. **`makeAllUrlsClickable()`**  
   â†’ Turned plain text URLs in Column A into proper HYPERLINK formulas so I could quickly click and see if the site looked promising. Saved tons of copy-paste pain.

3. **`checkUrlStatusWithResume()`** (the most important & painful one â€” I ran this many times)  
   â†’ Pings every URL in Column A â†’ writes status in Column B.  
   - 200 â†’ âœ… Active  
   - Other codes â†’ âš ï¸ Code: XXX  
   - Timeout/error â†’ ğŸ’€ Dead  
   Biggest life-saver: **resume logic** â€” skips rows that already have a status. Because Google gives only ~6 minutes per run â†’ I had to run it 6â€“10 times over days.  
   Also flushes every 5 rows so nothing gets lost.

4. **`shiftDataUp()`**  
   â†’ After moving active URLs to Column D (manually or with formulas), this quickly removes blank cells in Column D and shifts everything upward so I get one clean continuous list for the next scraping phase.

5. **`runProfessionalCleaning()`** (later attempt at one master function â€” but I mostly used the separate ones)  
   â†’ Tried to combine trimming, deduplication, color filtering into one run. Useful for learning, but in reality I did steps separately because debugging was easier.

## Whatâ€™s Next (The Exciting Part)

Now the fun begins:

1. Build the respectful crawler  
2. Extract novel names + author + download links  
3. Clean â†’ categorize â†’ push into Smart Urdu Novel Bank  
4. Let readers search â€œany novel in the worldâ€ (almost)

Iâ€™m genuinely excited.

If youâ€™re reading this â€” whether youâ€™re a fellow developer, an Urdu literature lover, or just someone who likes messy-to-clean stories â€” feel free to say hi or drop ideas.

Because this project is not just code.  
Itâ€™s years of love for Urdu stories finally getting organized so more people can read them easily.
